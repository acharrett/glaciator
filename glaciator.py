#!/usr/bin/python3
"""
Glaciator - a cli tool for uploading a directory structure to
AWS Glacier
https://github.com/acharrett/glaciator
"""

import json
import configparser
from pprint import pprint
import os
from datetime import datetime, timedelta
import pickle
import sys
from argparse import ArgumentParser
import pytz
import dateutil.parser
from botocore.utils import calculate_tree_hash
import boto3

def main():
    """ Main function """
    parser = ArgumentParser()
    parser = set_app_args(parser)
    app_args = parser.parse_args()

    glaciator_conf = GlaciatorConfig("glaciator.conf",app_args)
    all_vaults = glaciator_conf.vaults

    if app_args.vault is not False:
        all_vaults = [ app_args.vault ]

        if app_args.vault not in glaciator_conf.vault_config:
            print("No config for vault " + app_args.vault + " found")
            sys.exit(0)

    for vault_name in all_vaults:
        print("Vault is " + vault_name)
        vault_obj = GlacierVault(vault_name,glaciator_conf)
        vault_obj.print_upload_queue()
        vault_obj.print_delete_queue()
        vault_obj.check_inventory_against_local_files()
        vault_obj.check_inventory_for_duplicates()
        vault_obj.find_files_to_upload()
        if app_args.dry_run is True:
            vault_obj.print_upload_queue()
            vault_obj.print_delete_queue()
        else:
            vault_obj.upload_queued_files()
            vault_obj.delete_queued_files()
            vault_obj.save_state()

class GlacierVault():
    """ A class to represent an AWS Glacier vault """
    def __init__(self,vault_name,glaciator_conf):
        inventory_path = glaciator_conf.inventory_dir + "/" + vault_name + ".inventory"
        self.state_path = glaciator_conf.inventory_dir + "/" + vault_name + ".pickle"
        self.backup_dir_path = glaciator_conf.vault_config[vault_name]['path']
        self.never_delete = glaciator_conf.vault_config[vault_name]['never_delete']
        self.args = glaciator_conf.args
        self.vault_name = vault_name
        self.delete_time = glaciator_conf.delete_time
        self.region = glaciator_conf.aws_region
        self.skip = glaciator_conf.skip

        boto_kwargs = {}
        if glaciator_conf.aws_access_key != 'none':
            boto_kwargs['aws_access_key_id'] = glaciator_conf.aws_access_key

        if glaciator_conf.aws_secret_key != 'none':
            boto_kwargs['aws_secret_access_key'] = glaciator_conf.aws_secret_key

        if self.region != 'none':
            boto_kwargs['region_name'] = self.region


        self.glacier = boto3.client('glacier',**boto_kwargs)
        self.glacier_res = boto3.resource('glacier',**boto_kwargs)
        try:
            with open(inventory_path, encoding="utf8") as json_file:
                self.inventory = json.load(json_file)
        except IOError:
            self.inventory = {}
            self.inventory['ArchiveList'] = []

        try:
            with open(self.state_path, 'rb') as pickle_fh:
                self.vault_state = pickle.load(pickle_fh)
        except IOError:
            self.vault_state = {}

        if 'file_info' not in self.vault_state.keys():
            self.vault_state['file_info'] = {}

        if 'upload_queue' not in self.vault_state.keys():
            self.vault_state['upload_queue'] = []
        self.vault_state['upload_queue'] = []

        if 'delete_queue' not in self.vault_state.keys():
            self.vault_state['delete_queue'] = []

    def save_state(self):
        """ saves our state file to disk as vault_name.pickle """
        output_list = []
        for thing in self.vault_state['upload_queue']:
            if not os.path.exists(thing):
                self.vault_state['upload_queue'].remove(thing)
            if thing not in output_list:
                output_list.append(thing)

        self.vault_state['upload_queue'] = output_list

        output_list = []
        for thing in self.vault_state['delete_queue']:
            if thing not in output_list:
                output_list.append(thing)

        self.vault_state['delete_queue'] = output_list

        with open(self.state_path, 'wb') as pickle_fh:
            pickle.dump(self.vault_state, pickle_fh)

    def check_inventory_against_local_files(self):
        """ checks the inventory generated by AWS Glacier against
            the local files on disk """
        for vault_file in self.inventory['ArchiveList']:
            vault_file_name = self.get_filesystem_path(vault_file['ArchiveDescription'])
            if len(vault_file['ArchiveDescription']) > 0 and os.path.exists(vault_file_name):
                found_issue = False
                if self.args.skip_checksum is False:
                    with open(vault_file_name, 'rb') as vault_fh:
                        tree_hash = calculate_tree_hash(vault_fh)
                    if tree_hash != vault_file['SHA256TreeHash']:
                        print("hash mismatch " + vault_file_name + " I will delete and re-upload")
                        self.vault_state['delete_queue'].append(vault_file)
                        self.vault_state['upload_queue'].append(vault_file['ArchiveDescription'])
                        found_issue = True
                else:
                    filesize = os.path.getsize(vault_file_name)
                    if filesize != vault_file['Size']:
                        print("size mismatch " + vault_file_name + " I will delete and re-upload")
                        found_issue = True

                if found_issue is True:
                    self.vault_state['delete_queue'].append(vault_file)
                    self.vault_state['upload_queue'].append(vault_file['ArchiveDescription'])


            elif ((len(vault_file['ArchiveDescription']) > 0) and (self.never_delete is False)):
                #print("File is gone " + vault_file_name)
                vaulted_file_date = dateutil.parser.isoparse(vault_file['CreationDate'])
                #pprint(vaulted_file_date)
                #pprint(self.delete_time)
                if vaulted_file_date < self.delete_time:
                    #print("I will remove " + vault_file['ArchiveDescription'] + " from the vault")
                    #pprint(vault_file)
                    #pprint(vault_file['ArchiveDescription'])
                    if vault_file['ArchiveDescription'] in self.vault_state['file_info'].keys():
                        pprint(self.vault_state['file_info'][vault_file['ArchiveDescription']])
                    #else:
                    #    print(vault_file['ArchiveDescription'] + " not found in state")
                    if vault_file not in self.vault_state['delete_queue']:
                        self.vault_state['delete_queue'].append(vault_file)
                    #else:
                    #    print("it was already in the delete queue")

    def check_inventory_for_duplicates(self):
        """ Check AWS provided inventory for duplicate files """
        for vault_file in self.inventory['ArchiveList']:
            if vault_file in self.vault_state['delete_queue']:
                print("vault_file " + vault_file['ArchiveDescription'] + " already in delete queue")
                continue

            for check_file in self.inventory['ArchiveList']:
                if check_file in self.vault_state['delete_queue']:
                    continue

                if (vault_file['ArchiveDescription'] == check_file['ArchiveDescription']) \
                    and (vault_file['ArchiveId'] != check_file['ArchiveId']):

                    print("duplicate file in vault " + vault_file['ArchiveDescription'])

                    vault_file_archive_id = self.vault_state['file_info'][vault_file['ArchiveDescription']]['ArchiveId']
                    check_file_archive_id = self.vault_state['file_info'][check_file['ArchiveDescription']]['ArchiveId']

                    if vault_file['ArchiveId'] != vault_file_archive_id:
                        vault_file['duplicate'] = True
                        self.vault_state['delete_queue'].append(vault_file)
                    elif check_file['ArchiveId'] != check_file_archive_id:
                        check_file['duplicate'] = True
                        self.vault_state['delete_queue'].append(check_file)

    def file_metadata_from_inventory(self,name_in_vault):
        """ return the metadata for a specific archive from the AWS provided inventory """
        file_metadata = {}
        for vault_file in self.inventory['ArchiveList']:
            if vault_file['ArchiveDescription'] == name_in_vault:
                file_metadata = vault_file
        return file_metadata

    def find_files_to_upload(self):
        """ walk the directory structure to find files that need uploding """
        for dir_name, _, file_list in os.walk(self.backup_dir_path):
            dir_in_vault = dir_name.replace(self.backup_dir_path,"").lstrip('/')
            print(dir_in_vault)
            print(self.backup_dir_path)

            for file_name in file_list:
                name_on_filesystem = dir_name + "/" + file_name
                skip_check = self.is_file_in_skip(name_on_filesystem)

                if skip_check is True:
                    continue


                if len(dir_in_vault) > 0:
                    name_in_vault = dir_in_vault + "/" + file_name
                else:
                    name_in_vault = file_name

                print('possible upload ' + name_in_vault)
#                pprint(self.vault_state['file_info'].keys())
                pprint(self.vault_state['file_info'][name_in_vault])
                inventory_md = self.file_metadata_from_inventory(name_in_vault)
                pprint(inventory_md)

                if name_in_vault not in self.vault_state['file_info'].keys():
                    if 'ArchiveDescription' not in inventory_md.keys():
                        print("not in state file/inventory")
                        # File is not in either our state file or glacier's inventory
                        if name_on_filesystem not in self.vault_state['upload_queue']:
                            self.vault_state['upload_queue'].append(name_on_filesystem)

                elif 'ArchiveDescription' not in inventory_md.keys():
                    # consider not trying to upload if local statefile has upload within last 24 hours
                    if name_on_filesystem not in self.vault_state['upload_queue']:
                        self.vault_state['upload_queue'].append(name_on_filesystem)

    def is_file_in_skip(self,file_name):
        """ check the file name provided to see if we should skip uploading it """
        answer = False
        for skip_name in self.skip:
            if skip_name in file_name:
                answer = True
        return answer

    def upload_queued_files(self):
        """ upload all the files in the upload_queue list """
        for name_on_filesystem in self.vault_state['upload_queue']:
            failed_uploads = []
            if not os.path.exists(name_on_filesystem):
                self.vault_state['upload_queue'].remove(name_on_filesystem)

            else:
                name_in_vault = name_on_filesystem.replace(self.backup_dir_path + "/","")
                archive_id = self.upload_single_file(name_on_filesystem,name_in_vault)

                if len(archive_id) > 1:
                    self.vault_state['upload_queue'].remove(name_on_filesystem)
                    self.vault_state['file_info'][name_in_vault] = {}
                    self.vault_state['file_info'][name_in_vault]['ArchiveId'] = archive_id
                    self.vault_state['file_info'][name_in_vault]['upload_time'] = utc_now()
                else:
                    print("Upload failed " + name_on_filesystem)
                    failed_uploads.append(name_on_filesystem)

        self.vault_state['upload_queue'] = failed_uploads

    def delete_queued_files(self):
        """ delete archives from vault listed in the delete_queue list """
        print("----- deleting files -----")
        for vault_file in self.vault_state['delete_queue']:
            print("Deleting " + vault_file['ArchiveDescription'] + " " + vault_file['ArchiveId'] + " " + self.vault_name)

            # This is deleting the archive from the vault
            try:
                resp = self.glacier.delete_archive (
                    vaultName=self.vault_name,
                    archiveId=vault_file['ArchiveId']
                )
                pprint(resp)
            except:
                print("bang!")

            if vault_file['ArchiveDescription'] not in self.vault_state['file_info'].keys() \
               and 'duplicate' not in vault_file.keys():
                self.vault_state['file_info'][vault_file['ArchiveDescription']] = {}
                self.vault_state['file_info'][vault_file['ArchiveDescription']]['deleted_from_vault_time'] = utc_now()
                pprint(self.vault_state['file_info'][vault_file['ArchiveDescription']])

        self.vault_state['delete_queue'] = []

    def print_delete_queue(self):
        """ display the delete queue to screen """
        print("----- delete queue -----")
        for delq in self.vault_state['delete_queue']:
            pprint(delq)

    def print_upload_queue(self):
        """ display the upload queue to screen """
        print("----- upload queue -----")
        for upq in self.vault_state['upload_queue']:
            pprint(upq)

    def get_filesystem_path(self,archive_description):
        """ get the local filesystem path for this archive """
        vault_file_name = self.backup_dir_path
        if not archive_description.startswith('/'):
            vault_file_name += "/"
        vault_file_name += archive_description
        return vault_file_name

    def upload_single_file(self,name_on_filesystem,name_in_vault):
        """ upload a single file using mutlipart upload """
        print("Uploading " + name_on_filesystem)
        filesize = os.path.getsize(name_on_filesystem)
        with open(name_on_filesystem, mode='rb') as file_to_upload:
            part_size = 1024*1024*16

            response = self.glacier.initiate_multipart_upload(
                            vaultName=self.vault_name,
                            partSize=str(part_size),
                            archiveDescription=name_in_vault
                            )
            upload_id = response['uploadId']

            last_pos = 0
            file_to_upload.seek(0)

            while last_pos<filesize:
                next_pos = min(part_size,filesize-last_pos)
                next_pos = last_pos+next_pos
                next_minus_one = next_pos-1
                #upload_range = 'bytes %s-%s/*' % (last_pos,next_pos-1)
                upload_range = f"bytes {last_pos}-{next_minus_one}/*"
                upload_data = file_to_upload.read(min(part_size,filesize-last_pos))
                self.glacier.upload_multipart_part(
                    vaultName=self.vault_name,
                    uploadId=upload_id,
                    range=upload_range,
                    body=upload_data
                    )
                last_pos = next_pos

            file_to_upload.seek(0)
            file_checksum = calculate_tree_hash(file_to_upload)
            response = self.glacier.complete_multipart_upload(
                            vaultName=self.vault_name,
                            uploadId=upload_id,
                            archiveSize=str(filesize),
                            checksum=file_checksum
                            )

            pprint(response)
            archive_id=response['archiveId']
        return archive_id


class GlaciatorConfig():
    """ Class for our app config """
    def __init__(self,config_file,app_args):
        config = configparser.ConfigParser()
        config.read(config_file)

        self.args = app_args
        self.vaults = config.get("main","vaults").split()
        self.skip = config.get("main","skip").split()
        self.delete_months = int(config.get("main","purge_deleted_after_months"))
        self.delete_time = utc_now() - timedelta(days = (self.delete_months * 30))
        self.aws_region = config.get("aws","region-name")

        if config.has_option('aws', 'aws-access-key'):
            self.aws_access_key = config.get("aws","aws-access-key")
        else:
            self.aws_access_key = 'none'

        if config.has_option('aws', 'aws-secret-key'):
            self.aws_secret_key = config.get("aws","aws-secret-key")
        else:
            self.aws_secret_key = 'none'

        if config.has_option('aws', 'region-name'):
            self.aws_region = config.get("aws","region-name")
        else:
            self.aws_region = 'none'

        self.inventory_dir = config.get("main","inventory_dir")
        self.vault_config = {}
        for vault in self.vaults:
            self.vault_config[vault] = {}
            self.vault_config[vault]['path'] = config.get(vault,"path")
            self.vault_config[vault]['never_delete'] = False
            if config.has_option(vault, 'never_delete'):
                never_delete = config.get(vault,"never_delete")
                if never_delete == 'True':
                    self.vault_config[vault]['never_delete'] = True

def set_app_args(parser):
    """ set the command line arguments for glaciator """
    parser.add_argument(
        "-v",
        "--vault",
        help="single Glacier vault to use",
        default=False
    )

    parser.add_argument(
        "-d",
        "--dry-run",
        help="don't make any changes",
        default=False,
        action='store_true'
    )
    parser.add_argument(
        "-s",
        "--skip-checksum",
        help="disable checksum checking, use file size instead",
        default=False,
        action='store_true'
    )
    return parser

def utc_now():
    """ generate and return a datetime for right now, in UTC """
    return datetime.utcnow().replace(tzinfo=pytz.utc)

main()
